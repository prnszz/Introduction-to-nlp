{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Scratch build a transformer architecture decoder block\n",
    "We are going to achieve the same as (70%?maybe)  \n",
    "\"\"\"  \n",
    "from transformers import BertModel  \n",
    "model = BertModel.from_pretrained('bert-base-uncased')  \n",
    "\"\"\"  \n",
    "Let's start with a naive version first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "class NaiveTransformerLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __forward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build a module with pytorch, the preceding code is the basic structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "class NaiveTransformerLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dim = 768  # to set the 'parameter' dimention is 768\n",
    "    def __forward(self, x):\n",
    "        \"\"\"\n",
    "        input tensor x: nxd\n",
    "        output tensor out:nxd\n",
    "        \"\"\"\n",
    "        x = self.SelfAttention(x)\n",
    "        x = self.FFN(x)\n",
    "        out = x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the transformer architecture, we have to let the input go through 2 layers: SelfAttention layer and Feed Forward Network.  \n",
    "Then let's build the SelfAttention layer first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import math\n",
    "class NaiveTransformerLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dim = 768  \n",
    "        self.Wq = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.Wk = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.Wv = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.lm = nn.LayerNorm(self.dim)\n",
    "        \n",
    "    def SelfAttention(self, x):\n",
    "        \"\"\"\n",
    "        input x: nxd\n",
    "        output nxd\n",
    "        \"\"\"\n",
    "        Q = self.Wq(x)\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "        AttentionScore = torch.mm(Q, K.transpose(0, 1)) / math.sqrt(self.dim)\n",
    "        AttentionWeights = nn.Softmax(dim=1)(AttentionScore)\n",
    "        AttentionOutput = torch.mm(AttentionWeights, V)\n",
    "        output = self.lm(x + AttentionOutput)\n",
    "        return output\n",
    "\n",
    "    def __forward(self, x):\n",
    "        \"\"\"\n",
    "        input tensor x: nxd\n",
    "        output tensor out:nxd\n",
    "        \"\"\"\n",
    "        x = self.SelfAttention(x)\n",
    "        x = self.FFN(x)\n",
    "        out = x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have built the SelfAttention layer.\n",
    "\n",
    "1. Linear Transformations:\n",
    "   - $Q = W_q(x)$\n",
    "   - $K = W_k(x)$\n",
    "   - $V = W_v(x)$\n",
    "\n",
    "2. Attention Score Calculation:\n",
    "   - ${AttentionScore} = {Q \\cdot K^T}{\\sqrt{dim}}$\n",
    "\n",
    "3. Softmax Activation:\n",
    "   - $AttentionWeights = Softmax(AttentionScore)$\n",
    "\n",
    "4. Weighted Sum of Values:\n",
    "   - $AttentionOutput = AttentionWeights \\cdot V$\n",
    "\n",
    "5. Residual Connection and Layer Normalization:\n",
    "   - $output = lm(x + AttentionOutput)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import math\n",
    "class NaiveTransformerLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dim = 768  \n",
    "        self.Wq = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.Wk = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.Wv = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.lm = nn.LayerNorm(self.dim)\n",
    "        self.ffn1 = nn.Linear(self.dim, self.dim*4)\n",
    "        self.ffn2 = nn.Linear(self.dim*4, self.dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.lm_ffn = nn.LayerNorm(self.dim)\n",
    "\n",
    "    def SelfAttention(self, x):\n",
    "        \"\"\"\n",
    "        input x: nxd\n",
    "        output nxd\n",
    "        \"\"\"\n",
    "        Q = self.Wq(x)\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "        AttentionScore = torch.mm(Q, K.transpose(0, 1)) / math.sqrt(self.dim)\n",
    "        AttentionWeights = nn.Softmax(dim=1)(AttentionScore)\n",
    "        AttentionOutput = torch.mm(AttentionWeights, V)\n",
    "        AttentionOutput = self.lm(x + AttentionOutput)\n",
    "        return AttentionOutput\n",
    "    \n",
    "    def FFN(self, x):\n",
    "        hidden = self.ffn1(x)\n",
    "        hidden = self.act(hidden)\n",
    "        output = self.ffn2(hidden)\n",
    "        output = self.lm_ffn(x + output)\n",
    "        return output\n",
    "\n",
    "    def __forward(self, x):\n",
    "        \"\"\"\n",
    "        input tensor x: nxd\n",
    "        output tensor out:nxd\n",
    "        \"\"\"\n",
    "        x = self.SelfAttention(x)\n",
    "        x = self.FFN(x)\n",
    "        out = x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have built the Feed Forward Layer.  \n",
    "1. Linear Transformations and Activation Function:\n",
    "   - $hidden = W_1(x)+b_1$   mapping from dimension dim to dim*4\n",
    "   - $hidden = GELU(hidden)$ \n",
    "   - $output = W_2(hidden)+b_2$   mapping from dimension dim*4 to dim\n",
    "\n",
    "2. Residual Connection and Layer Normalization:\n",
    "   - $output = lm_{ffn}(x + output)$  use another ffn layer becuase of using another set of parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a naive version of transformer architecture.  \n",
    "Let's take a look what we are lacking for a true transformer architecture.  \n",
    "- batch\n",
    "- dropout \n",
    "- multi-headattention\n",
    "- attention mask / padding mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #batch\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import math\n",
    "class NaiveTransformerLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dim = 768  \n",
    "        self.Wq = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.Wk = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.Wv = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.lm = nn.LayerNorm(self.dim)\n",
    "        self.ffn1 = nn.Linear(self.dim, self.dim*4)\n",
    "        self.ffn2 = nn.Linear(self.dim*4, self.dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.lm_ffn = nn.LayerNorm(self.dim)\n",
    "\n",
    "    def SelfAttention(self, x):\n",
    "        \"\"\"\n",
    "        input x: bxnxd\n",
    "        output bxnxd\n",
    "        \"\"\"\n",
    "        Q = self.Wq(x)\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "        AttentionScore = torch.bmm(Q, K.transpose(1, 2)) / math.sqrt(self.dim) #\n",
    "        AttentionWeights = nn.Softmax(dim=2)(AttentionScore)\n",
    "        AttentionOutput = torch.bmm(AttentionWeights, V) #\n",
    "        AttentionOutput = self.lm(x + AttentionOutput)\n",
    "        return AttentionOutput\n",
    "    \n",
    "    def FFN(self, x):\n",
    "        hidden = self.ffn1(x)\n",
    "        hidden = self.act(hidden)\n",
    "        output = self.ffn2(hidden)\n",
    "        output = self.lm_ffn(x + output)\n",
    "        return output\n",
    "\n",
    "    def __forward(self, x):\n",
    "        \"\"\"\n",
    "        input x: bxnxd\n",
    "        output out:bxnxd\n",
    "        \"\"\"\n",
    "        x = self.SelfAttention(x)\n",
    "        x = self.FFN(x)\n",
    "        out = x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop out\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import math\n",
    "class NaiveTransformerLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dim = 768  \n",
    "        self.att_drop_prob = 0.1 #\n",
    "        self.state_drop_prob = 0.5 #\n",
    "        self.Wq = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.Wk = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.Wv = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.lm = nn.LayerNorm(self.dim)\n",
    "        self.ffn1 = nn.Linear(self.dim, self.dim*4)\n",
    "        self.ffn2 = nn.Linear(self.dim*4, self.dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.lm_ffn = nn.LayerNorm(self.dim)\n",
    "        self.att_drop = nn.Dropout(self.att_drop_prob) #\n",
    "        self.state_drop = nn.Dropout(self.state_drop_prob) #\n",
    "\n",
    "    def SelfAttention(self, x):\n",
    "        \"\"\"\n",
    "        input x: bxnxd\n",
    "        output bxnxd\n",
    "        \"\"\"\n",
    "        Q = self.Wq(x)\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "        AttentionScore = torch.bmm(Q, K.transpose(1, 2)) / math.sqrt(self.dim)\n",
    "        AttentionWeights = nn.Softmax(dim=2)(AttentionScore)\n",
    "        AttentionWeights = self.att_drop(AttentionWeights) #\n",
    "        AttentionOutput = torch.bmm(AttentionWeights, V)\n",
    "        AttentionOutput = self.state_drop(AttentionOutput) #\n",
    "        AttentionOutput = self.lm(x + AttentionOutput)\n",
    "        return AttentionOutput\n",
    "    \n",
    "    def FFN(self, x):\n",
    "        hidden = self.ffn1(x)\n",
    "        hidden = self.act(hidden)\n",
    "        output = self.ffn2(hidden)\n",
    "        output = self.state_drop(output) #\n",
    "        output = self.lm_ffn(x + output)\n",
    "        return output\n",
    "\n",
    "    def __forward(self, x):\n",
    "        \"\"\"\n",
    "        input x: bxnxd\n",
    "        output out:bxnxd\n",
    "        \"\"\"\n",
    "        x = self.SelfAttention(x)\n",
    "        x = self.FFN(x)\n",
    "        out = x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have done the work for batch and drop out.  \n",
    "Let's go to deal with multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import math\n",
    "class MultiTransformerLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dim = 768  \n",
    "        self.att_drop_prob = 0.1\n",
    "        self.state_drop_prob = 0.5\n",
    "        self.num_heads = 12 #\n",
    "        self.size_per_head = self.dim // self.num_heads # 64\n",
    "        self.Wq = nn.Linear(self.dim, self.num_heads * self.size_per_head, bias=False) #\n",
    "        self.Wk = nn.Linear(self.dim, self.num_heads * self.size_per_head, bias=False) #\n",
    "        self.Wv = nn.Linear(self.dim, self.num_heads * self.size_per_head, bias=False) #\n",
    "        self.map = nn.Linear(self.num_heads * self.size_per_head, self.dim)\n",
    "        self.lm = nn.LayerNorm(self.dim)\n",
    "        self.ffn1 = nn.Linear(self.dim, self.dim*4)\n",
    "        self.ffn2 = nn.Linear(self.dim*4, self.dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.lm_ffn = nn.LayerNorm(self.dim)\n",
    "        self.att_drop = nn.Dropout(self.att_drop_prob)\n",
    "        self.state_drop = nn.Dropout(self.state_drop_prob)\n",
    "\n",
    "    def SelfAttention(self, x):\n",
    "        \"\"\"\n",
    "        input x: bxnxd\n",
    "        Q, K, V : bxnx(hxs) -> bxnxhxs -> bxhxnxs \n",
    "        output bxnxd\n",
    "        \"\"\"\n",
    "        new_size = x.size()[:-1] + (self.num_heads, self.size_per_head) # b, n, h, s\n",
    "        Q = self.Wq(x).view(*new_size).permute(0, 2, 1, 3) #\n",
    "        K = self.Wk(x).view(*new_size).permute(0, 2, 1, 3) #\n",
    "        V = self.Wv(x).view(*new_size).permute(0, 2, 1, 3) #\n",
    "        AttentionScore = torch.matmul(Q, K.transpose(2, 3)) / math.sqrt(self.dim)\n",
    "        AttentionWeights = nn.Softmax(dim=3)(AttentionScore)\n",
    "        AttentionWeights = self.att_drop(AttentionWeights)\n",
    "        AttentionOutput = torch.matmul(AttentionWeights, V)\n",
    "        AttentionOutput = AttentionOutput.permute(0, 2, 1, 3) # bxhxnxs -> bxnxhxs\n",
    "        AttentionOutput = self.map(AttentionOutput) # bxnxhxs -> bxnxd\n",
    "        AttentionOutput = self.state_drop(AttentionOutput)\n",
    "        AttentionOutput = self.lm(x + AttentionOutput)\n",
    "        return AttentionOutput\n",
    "    \n",
    "    def FFN(self, x):\n",
    "        hidden = self.ffn1(x)\n",
    "        hidden = self.act(hidden)\n",
    "        output = self.ffn2(hidden)\n",
    "        output = self.state_drop(output)\n",
    "        output = self.lm_ffn(x + output)\n",
    "        return output\n",
    "\n",
    "    def __forward(self, x):\n",
    "        \"\"\"\n",
    "        input x: bxnxd\n",
    "        output out:bxnxd\n",
    "        \"\"\"\n",
    "        x = self.SelfAttention(x)\n",
    "        x = self.FFN(x)\n",
    "        out = x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for keep going with me to here! We are very closed to complish our coding. So far, we have already got almost every part of a transformer decoder block. The only thing we left here is attention mask / padding mask. Let's finish it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import math\n",
    "class MultiTransformerLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dim = 768  \n",
    "        self.att_drop_prob = 0.1\n",
    "        self.state_drop_prob = 0.5\n",
    "        self.num_heads = 12 #\n",
    "        self.size_per_head = self.dim // self.num_heads # 64\n",
    "        self.Wq = nn.Linear(self.dim, self.num_heads * self.size_per_head, bias=False) #\n",
    "        self.Wk = nn.Linear(self.dim, self.num_heads * self.size_per_head, bias=False) #\n",
    "        self.Wv = nn.Linear(self.dim, self.num_heads * self.size_per_head, bias=False) #\n",
    "        self.map = nn.Linear(self.num_heads * self.size_per_head, self.dim)\n",
    "        self.lm = nn.LayerNorm(self.dim)\n",
    "        self.ffn1 = nn.Linear(self.dim, self.dim*4)\n",
    "        self.ffn2 = nn.Linear(self.dim*4, self.dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.lm_ffn = nn.LayerNorm(self.dim)\n",
    "        self.att_drop = nn.Dropout(self.att_drop_prob)\n",
    "        self.state_drop = nn.Dropout(self.state_drop_prob)\n",
    "\n",
    "    def calc_mask_score(self, attention_mask):\n",
    "        \"\"\"\n",
    "        input bxn\n",
    "        output bxhxnxn\n",
    "        \"\"\"\n",
    "        mask_score = torch.zeros(attention_mask.size(0), self.num_heads, attention_mask.size(1), attention_mask.size(1))\n",
    "        mask_score = mask_score + attention_mask[:, None, None, :]\n",
    "        mask_score = (1.0 - mask_score) * -10000.\n",
    "        return mask_score\n",
    "\n",
    "    def SelfAttention(self, x, attention_mask):\n",
    "        \"\"\"\n",
    "        input x: bxnxd\n",
    "        Q, K, V : bxnx(hxs) -> bxnxhxs -> bxhxnxs\n",
    "        attention_mask: bxn\n",
    "            1 -> normal token\n",
    "            0 -> masked token \n",
    "        output bxnxd\n",
    "        \"\"\"\n",
    "        new_size = x.size()[:-1] + (self.num_heads, self.size_per_head) # b, n, h, s\n",
    "        Q = self.Wq(x).view(*new_size).permute(0, 2, 1, 3) #\n",
    "        K = self.Wk(x).view(*new_size).permute(0, 2, 1, 3) #\n",
    "        V = self.Wv(x).view(*new_size).permute(0, 2, 1, 3) #\n",
    "        AttentionScore = torch.matmul(Q, K.transpose(2, 3)) / math.sqrt(self.dim)\n",
    "        AttentionScore = AttentionScore + self.calc_mask_score(attention_mask)\n",
    "        AttentionWeights = nn.Softmax(dim=3)(AttentionScore)\n",
    "        AttentionWeights = self.att_drop(AttentionWeights)\n",
    "        AttentionOutput = torch.matmul(AttentionWeights, V)\n",
    "        AttentionOutput = AttentionOutput.permute(0, 2, 1, 3) # bxhxnxs -> bxnxhxs\n",
    "        AttentionOutput = self.map(AttentionOutput) # bxnxhxs -> bxnxd\n",
    "        AttentionOutput = self.state_drop(AttentionOutput)\n",
    "        AttentionOutput = self.lm(x + AttentionOutput)\n",
    "        return AttentionOutput\n",
    "    \n",
    "    def FFN(self, x):\n",
    "        hidden = self.ffn1(x)\n",
    "        hidden = self.act(hidden)\n",
    "        output = self.ffn2(hidden)\n",
    "        output = self.state_drop(output)\n",
    "        output = self.lm_ffn(x + output)\n",
    "        return output\n",
    "\n",
    "    def __forward(self, x):\n",
    "        \"\"\"\n",
    "        input x: bxnxd\n",
    "        output out:bxnxd\n",
    "        \"\"\"\n",
    "        x = self.SelfAttention(x)\n",
    "        x = self.FFN(x)\n",
    "        out = x\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
